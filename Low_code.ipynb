{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWRJWC4wECS3",
        "outputId": "21094c8d-087d-4a43-daac-3c2a95a85f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import google.generativeai as genai\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "from google.colab import drive\n",
        "import time\n",
        "import google.api_core.exceptions\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUHPhac1EDhi",
        "outputId": "6c654157-ec67-445e-88d6-edf53b039a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract text from PDF\n",
        "# def extract_text_from_pdf(pdf_path):\n",
        "#     text = \"\"\n",
        "#     with open(pdf_path, 'rb') as file:\n",
        "#         pdf = PyPDF2.PdfReader(file)\n",
        "#         for page_num in range(len(pdf.pages)):\n",
        "#             page = pdf.pages[page_num]\n",
        "#             text += page.extract_text()\n",
        "#     return text\n",
        "def extract_text_from_multiple_pdfs(pdf_paths):\n",
        "    combined_text = \"\"\n",
        "    for pdf_path in pdf_paths:\n",
        "        print(pdf_path)\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(pdf.pages)):\n",
        "                page = pdf.pages[page_num]\n",
        "                combined_text += page.extract_text()\n",
        "    return combined_text\n",
        "\n",
        "\n",
        "# pdf_path = '/content/drive/MyDrive/UNH_Hackathon/US_Mercy.pdf'\n",
        "# text = extract_text_from_pdf(pdf_path)\n",
        "# print(\"Extracted Text from PDF:\\n\", text[:500], \"\\n...\")  # Print first 500 chars for brevity\n",
        "\n",
        "# List of PDF paths\n",
        "pdf_paths = [\n",
        "    '/content/drive/MyDrive/UNH_Hackathon/US_Mercy.pdf',\n",
        "    '/content/drive/MyDrive/UNH_Hackathon/A_Decade_of_Surgery_Aboard_the_U.S._COMFORT.pdf',\n",
        "    '/content/drive/MyDrive/UNH_Hackathon/Hospital_ships_adrift:_Part_2:_The_role_of_U.S._Navy_hospital_shi.pdf',\n",
        "    '/content/drive/MyDrive/UNH_Hackathon/Sea_Power:_The_U.S._Navy_and_Foreign_Policy:_Council_on_Foreign_Relations.pdf',\n",
        "    '/content/drive/MyDrive/UNH_Hackathon/US_Navy_Ship-Based_Disaster_Response:_Lessons_Learned_-_PMC.pdf'\n",
        "]\n",
        "\n",
        "\n",
        "# Extract text from all PDFs\n",
        "combined_text = extract_text_from_multiple_pdfs(pdf_paths)\n",
        "print(\"Extracted Combined Text from PDFs:\\n\", combined_text[:500], \"\\n...\")  # Print first 500 chars for brevity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlewwVXROYYG",
        "outputId": "00da894d-8e24-4def-c9ec-a96b6d3c01c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/UNH_Hackathon/US_Mercy.pdf\n",
            "/content/drive/MyDrive/UNH_Hackathon/A_Decade_of_Surgery_Aboard_the_U.S._COMFORT.pdf\n",
            "/content/drive/MyDrive/UNH_Hackathon/Hospital_ships_adrift:_Part_2:_The_role_of_U.S._Navy_hospital_shi.pdf\n",
            "/content/drive/MyDrive/UNH_Hackathon/Sea_Power:_The_U.S._Navy_and_Foreign_Policy:_Council_on_Foreign_Relations.pdf\n",
            "/content/drive/MyDrive/UNH_Hackathon/US_Navy_Ship-Based_Disaster_Response:_Lessons_Learned_-_PMC.pdf\n",
            "Extracted Combined Text from PDFs:\n",
            " Humanitarian Assistance and Disaster\n",
            "Relief Aboard the USNS Mercy (TAH-19)\n",
            "CDR Matthew T. Provencher, MD, and CDR Trent D. Douglas, MD\n",
            "The USNS Mercy , one of the two hospital ships in the United States Navy, has provided disaster\n",
            "relief and humanitarian assistance for multiple natur al disasters around the world. As a self-sustaining\n",
            "1000-bed hospital, the USNS Mercy provides a full complement of surgical and medical capabilities to\n",
            "care for the sick and injured in a mobile platform enviro nmen \n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(len(text))"
      ],
      "metadata": {
        "id": "wak9ypsVXInw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Model Configuration + Prompt Engineering\n",
        "def setup_model():\n",
        "    # API\n",
        "    genai.configure(api_key=\"AIzaSyCxkUFNQKbkfcE0Mj1EDmzNfp7Tr99CAsU\")\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": 1,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 0,\n",
        "        \"max_output_tokens\": 8192,\n",
        "    }\n",
        "\n",
        "    safety_settings = [\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "    ]\n",
        "\n",
        "    system_instruction = \"\"\"\n",
        "    Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "    Write a response that appropriately completes the request.\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=\"gemini-1.5-pro-latest\",\n",
        "        generation_config=generation_config,\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "model = setup_model()\n",
        "\n",
        "# Function to generate a response based on context and instruction\n",
        "def generate_response(instruction, context):\n",
        "    # Combine context and instruction for the prompt\n",
        "    prompt = f\"Instruction: {instruction}\\n\\nInput: {context}\"\n",
        "\n",
        "\n",
        "\n",
        "    # Generate content using the configured Gemini API model\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Ensure `candidates` is converted to a list before accessing its content\n",
        "    candidates = list(response.candidates)\n",
        "\n",
        "    generated_text = candidates[0].content.parts if candidates else \"No response generated.\"\n",
        "\n",
        "    if candidates:\n",
        "      str_g=str(generated_text)\n",
        "      str_g=str_g.split('[text: \"')[1]\n",
        "      str_g=str_g.split()\n",
        "      str_g= str_g[:-2]\n",
        "      str_g=\" \".join(str_g)\n",
        "      generated_text=str_g\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Main function to interact via command line\n",
        "def main():\n",
        "    print(\"Contextual Text Prompting\")\n",
        "\n",
        "    # User input: Provide context and a question/instruction\n",
        "    context_text = \"dog is man's best friend\"\n",
        "    instruction = \"write a short 20 word story about dogs\"\n",
        "\n",
        "    # Generate and display response\n",
        "    print(\"\\nGenerating response... Please wait.\\n\")\n",
        "    response =generate_response(context_text, instruction)\n",
        "\n",
        "    print(\"\\nGenerated Response:\\n\")\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "RRffoPv5Oabr",
        "outputId": "da9bccd4-2036-4e76-9a04-0c6c45c8822a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contextual Text Prompting\n",
            "\n",
            "Generating response... Please wait.\n",
            "\n",
            "\n",
            "Generated Response:\n",
            "\n",
            "The playful puppy, a whirlwind of fur, chased butterflies in the park, bringing joy to all who watched.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Prompting the Fine-tuned Model\n",
        "import csv\n",
        "import time\n",
        "import google.api_core.exceptions\n",
        "\n",
        "#--------------------------------------------\n",
        "# def prompt_model_with_retry(instruct, context_text=None, max_retries=5):\n",
        "#     retries = 0\n",
        "#     while retries < max_retries:\n",
        "#         try:\n",
        "#             response = generate_response(instruct, context_text)  # Capture the response here\n",
        "#             return response  # Return the generated response\n",
        "#         except google.api_core.exceptions.TooManyRequests:\n",
        "#             retries += 1\n",
        "#             wait_time = 2 ** retries  # Exponential backoff\n",
        "#             print(f\"Rate limit hit, retrying in {wait_time} seconds...\")\n",
        "#             time.sleep(wait_time)\n",
        "#         except Exception as e:\n",
        "#             print(f\"An error occurred: {e}\")\n",
        "#             break\n",
        "#     return \"No response generated due to error or rate limit.\"  # Return a fallback message if failed\n",
        "\n",
        "\n",
        "def prompt_model_with_retry(instruct, context_text=None, max_retries=5):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = generate_response(instruct, context_text)  # Capture the response here\n",
        "            return response  # Return the generated response\n",
        "        except google.api_core.exceptions.TooManyRequests:\n",
        "            retries += 1\n",
        "            wait_time = 2 ** retries  # Exponential backoff\n",
        "            print(f\"Rate limit hit, retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "    return \"No response generated due to error or rate limit.\"  # Return a fallback message if failed\n",
        "# List to store question-answer pairs\n",
        "qa_pairs = []"
      ],
      "metadata": {
        "id": "tUETUdkpRBhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the primary objectives and differences between the USNS Mercy and USNS Comfort hospital ships during humanitarian missions, and how have these objectives evolved over time?\",\n",
        "    \"Describe how hospital ships like the USNS Mercy and Comfort have supported disaster relief efforts. Include examples from both recent and historical missions.\",\n",
        "    \"How does the U.S. Navy’s use of hospital ships contribute to the nation’s foreign policy and soft power objectives? Cite specific missions and outcomes.\",\n",
        "    \"What are the main logistical and operational challenges hospital ships face when deployed for disaster relief missions? How have these challenges been addressed in past deployments?\",\n",
        "    \"Explain the role of non-governmental organizations (NGOs) and partner nations in U.S. Navy hospital ship missions, such as Pacific Partnership. How does this collaboration enhance mission effectiveness?\",\n",
        "    \"What is the significance of 'single-stage surgical intervention' on missions conducted by U.S. Navy hospital ships, and why is it often prioritized?\",\n",
        "    \"Identify the types of medical conditions most commonly treated aboard U.S. hospital ships during humanitarian missions. How does the medical staff prepare to address these conditions?\",\n",
        "    \"Discuss the decision-making process involved in selecting countries for hospital ship deployments, particularly for missions like Pacific Partnership and Continuing Promise. What strategic factors influence these choices?\",\n",
        "    \"What have been some key lessons learned from past U.S. Navy ship-based disaster responses, and how have these influenced subsequent missions?\",\n",
        "    \"How do the capabilities of U.S. Navy hospital ships compare with those of land-based hospitals in terms of trauma care and surgical capacity? Provide specific details about facilities and resources.\",\n",
        "    \"Describe the planning and screening process for surgical cases during a hospital ship mission. What considerations are made to ensure that care provided is both effective and culturally sensitive?\",\n",
        "    \"How do hospital ship missions support infrastructure capacity building in host nations, and what role does medical training play in these efforts?\",\n",
        "    \"Examine the challenges and ethical considerations of providing short-term medical care on hospital ships, especially in regions with limited healthcare infrastructure. How are follow-up care needs addressed?\",\n",
        "    \"What role did the U.S. Navy hospital ships play in responses to natural disasters like the 2004 Banda Aceh tsunami, and how did this response shape future deployment strategies?\",\n",
        "    \"How do hospital ship missions contribute to diplomatic relations between the U.S. and the host countries? Provide examples where hospital ship missions have either improved or faced challenges in diplomatic relations.\"\n",
        "];\n",
        "\n",
        "# for question in questions:\n",
        "#     answer = prompt_model_with_retry(question, context_text=combined_text)\n",
        "#     qa_pairs.append({\"Question\": question, \"Answer\": answer})\n",
        "\n",
        "\n",
        "#------------\n",
        "\n",
        "# Function to process a batch of questions\n",
        "def process_batch(batch):\n",
        "    for question in batch:\n",
        "        answer = prompt_model_with_retry(question, context_text=combined_text)\n",
        "        qa_pairs.append({\"Question\": question, \"Answer\": answer})\n",
        "        time.sleep(5)  # Small delay between each question\n",
        "\n",
        "# Split questions into batches of 5 (or any manageable number)\n",
        "batch_size = 5\n",
        "batches = [questions[i:i + batch_size] for i in range(0, len(questions), batch_size)]\n",
        "\n",
        "# Process each batch with a delay in between\n",
        "for batch in batches:\n",
        "    process_batch(batch)\n",
        "    print(\"Batch processed. Waiting before next batch...\")\n",
        "    time.sleep(30)  # Longer delay between batches to avoid rate limits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z74uF18C3e7d",
        "outputId": "5df1c135-089c-454e-ff7e-26985e687c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1323.42ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2260.06ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1001.13ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 967.06ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 891.17ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 915.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1277.17ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 889.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1002.66ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1118.30ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 915.86ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1319.96ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1196.60ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 945.35ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 914.82ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1522.12ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1370.92ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1072.03ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 992.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1373.44ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 917.74ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 865.98ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1099.39ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 892.23ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1072.11ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n",
            "Batch processed. Waiting before next batch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1017.78ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1193.40ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1222.82ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 996.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 965.22ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1830.76ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1674.81ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 944.53ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1625.68ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1269.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1094.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1016.91ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 894.49ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 914.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1221.41ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 940.95ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 915.05ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1143.36ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 941.38ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 991.72ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1074.37ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 839.96ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2027.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 915.68ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1294.68ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n",
            "Batch processed. Waiting before next batch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 966.99ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 813.85ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 942.34ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1016.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 872.12ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: 500 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: TypeError: Failed to fetch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1092.78ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1320.13ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 923.55ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 992.98ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 991.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 915.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 864.76ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1017.96ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 994.59ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1528.85ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 972.81ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 916.10ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1041.69ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 993.13ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 890.73ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1171.79ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 2 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1021.27ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 4 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 891.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 8 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1016.94ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 16 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 890.03ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limit hit, retrying in 32 seconds...\n",
            "Batch processed. Waiting before next batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the question-answer pairs to a CSV file\n",
        "csv_file_path = \"/content/drive/MyDrive/UNH_Hackathon/questions_answers.csv\"\n",
        "with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
        "    fieldnames = [\"Question\", \"Answer\"]\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    writer.writerows(qa_pairs)\n",
        "\n",
        "print(f\"CSV file created successfully at {csv_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb3gdA4BG2_a",
        "outputId": "02ae62e8-9359-4654-e060-fffc8beede9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully at /content/drive/MyDrive/UNH_Hackathon/questions_answers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "# Answer: The USNS COMFORT was primarily commissioned as a combat support hospital ship in 1987. However, its role has evolved to focus more on humanitarian assistance and disaster relief missions. Since 2007, the ship has participated in multiple humanitarian missions, such as \"Continuing Promise,\" aimed at strengthening partnerships and providing medical care in different countries.\n",
        "instruction= \"What is the primary purpose of the USNS COMFORT, and how has its role evolved over time?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "j7Dq73llVs3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "# Answer: From 2007 to 2019, the USNS COMFORT conducted six humanitarian assistance and disaster relief missions, spanning 19 countries. The countries most frequently visited were Colombia (seven visits) and Haiti (five visits).\n",
        "instruction= \"How many humanitarian assistance and disaster relief missions did the USNS COMFORT conduct from 2007 to 2019, and which countries were visited most frequently?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "GZYMyGcyVxbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "# Answer: The surgical specialties consistently represented on the missions were general surgery, ophthalmology, plastic surgery, orthopedic surgery, and oral/maxillofacial surgery. Orthopedic surgery saw a significant decline in case volume over time, except for a spike during the 2018 mission.\n",
        "instruction= \"What surgical specialties were consistently represented during the missions, and which specialty saw a significant decline in volume over time?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "vNIckvnsVxwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "# Answer: The total surgical case volume for the missions from 2009 to 2019 was 5,142. General surgery, ophthalmology, and plastic surgery maintained consistent case volumes, while orthopedic surgery experienced a decrease. The diversity of surgical cases varied year by year, with certain countries contributing to higher volumes in specific specialties. The most notable cases included general surgery in Panama (92 cases in 2009) and ophthalmology in Honduras (65 cases in 2018).\n",
        "instruction= \"How did the surgical case volume vary by specialty and year across the missions, and what trends were identified?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "a6pS7lEZVyju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5\n",
        "# Answer: The most common reason for unanticipated returns to the operating room (OR) was the evacuation of a hematoma following inguinal hernia repair. The rates of unanticipated returns to the OR were 0.35% in 2009, 0.18% in 2011, 0.41% in 2015, 0.17% in 2018, and 0.74% in 2019.\n",
        "instruction= \"What was the most common reason for an unanticipated return to the operating room during the missions, and how frequently did these incidents occur?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "gpdZGtjgVzAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "# Answer: The paper recommends that historical case volume data should guide staffing decisions for future missions. Specifically, specialties with consistently low case volumes, such as orthopedic surgery, should be deployed only to countries where pre-identified cases are available. Additionally, partnerships with local healthcare providers can help improve patient follow-up and case selection.\n",
        "nstruction= \"What recommendations are made in the paper for improving future surgical missions in terms of staffing and case selection?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "QcuA9C-pVzjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7\n",
        "# Answer: Partnerships with local surgeons, healthcare systems, and NGOs were seen as crucial for success. These partnerships helped in screening patients, providing follow-up care, and enhancing surgical readiness. Local collaboration also helped build capacity and strengthen relationships with partner nations.\n",
        "instruction= \"How did partnerships with local healthcare systems and NGOs play a role in the success or challenges of the surgical missions?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "_TQxu2TsV0I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8\n",
        "# Answer: Logistical challenges included limited pre-deployment site survey time (less than two months in some cases), difficulty in matching ship capabilities with the partner nation’s surgical needs, and the inability to include complex surgeries like bariatric, cardiothoracic, and transplant surgery due to insufficient planning.\n",
        "instruction= \"What are the key logistical challenges mentioned in the paper related to the planning and execution of surgical missions aboard the USNS COMFORT?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "Fw1iU7MeV1RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9\n",
        "# Answer: Participation in USNS COMFORT missions helped maintain and enhance the readiness of military surgeons by exposing them to diverse surgical experiences. However, alternative methods to maintain combat readiness are being explored, such as increasing case volume through global health engagements (GHEs) and collaborating with local healthcare systems during missions.\n",
        "instruction= \"How did the missions impact the training and readiness of military surgeons, and what alternative methods are suggested for maintaining combat readiness?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "bdjgt1vqV2o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10\n",
        "# Answer: The paper emphasizes the need for standardized and thorough surgical logs. Standard Form 516 should be digitized, including current procedural terminology (CPT) codes for all cases. The logs should also record patient demographic information and complications, whether or not they result in a return to the OR, to enhance patient follow-up and inform future mission planning.\n",
        "instruction= \"What are the recommendations for record-keeping and data management to improve the documentation of surgical cases during future missions?\"\n",
        "\n",
        "prompt_model(instruction, text)"
      ],
      "metadata": {
        "id": "Z9MEh41vV29z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}